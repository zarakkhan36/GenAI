{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zarakkhan36/GenAI/blob/main/Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
      "metadata": {
        "id": "b076bd1a-b236-4fbc-953d-8295b25122ae"
      },
      "source": [
        "Text Generation Using LSTM on Project Gutenberg Training Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zhBQqEZKNV4Q"
      },
      "id": "zhBQqEZKNV4Q"
    },
    {
      "cell_type": "markdown",
      "id": "658a95da-9645-4bcf-bd9d-4b95a4b6f582",
      "metadata": {
        "id": "658a95da-9645-4bcf-bd9d-4b95a4b6f582"
      },
      "source": [
        "In this notebook, we'll walk through the steps required to train your own LSTM on the recipes dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0d56cc-4773-4029-97d8-26f882ba79c9",
      "metadata": {
        "id": "4e0d56cc-4773-4029-97d8-26f882ba79c9"
      },
      "outputs": [],
      "source": [
        "# Zarak Khan\n",
        "# Develop LSTM to generate Shakespearean text by training on project Gutenberg training data\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jxe-dOz1R4kz"
      },
      "id": "Jxe-dOz1R4kz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1",
      "metadata": {
        "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 200\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7716fac-0010-49b0-b98e-53be2259edde",
      "metadata": {
        "id": "b7716fac-0010-49b0-b98e-53be2259edde"
      },
      "source": [
        "## 1. Load the data <a name=\"load\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0O8-7aHJBSt9",
        "outputId": "c13ddd8b-7f02-4712-fd67-a0ccf1dd164f"
      },
      "id": "0O8-7aHJBSt9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
      "metadata": {
        "tags": [],
        "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e7c3f6-a7ec-43f6-abe8-0623fb1ca0e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK 1041 ***\r\n",
            "THE SONNETS\r\n",
            "\r\n",
            "by William Shakespeare\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "I\r\n",
            "\r\n",
            "From fairest creatures we desire increase,\r\n",
            "That thereby beauty’s rose might never die,\r\n",
            "But as the riper should by time decease,\r\n",
            "His tender heir might bear his memory:\r\n",
            "But thou, contracted to thine own bright eyes,\r\n",
            "Feed’st thy light’s flame with self-substantial fuel,\r\n",
            "Making a famine where abundance lies,\r\n",
            "Thyself thy foe, to thy sweet self too cruel:\r\n",
            "Thou that art now the world’s fresh ornament,\r\n",
            "And only herald to the gaudy spring,\r\n",
            "Within thine own bud buriest thy content,\r\n",
            "And tender churl mak’st waste in niggarding:\r\n",
            "    Pity the world, or else this glutton be,\r\n",
            "    To eat the world’s due, by the grave and thee.\r\n",
            "\r\n",
            "II\r\n",
            "\r\n",
            "When forty winters shall besiege thy brow,\r\n",
            "And dig deep trenches in thy beauty’s field,\r\n",
            "Thy youth’s proud livery so gazed on now,\r\n",
            "Will be a tatter’d weed of small worth held:\r\n",
            "Then being asked, where all thy beauty lies,\r\n",
            "Where all the treasure of thy lusty days;\r\n",
            "To say, within thine own deep sunken eyes,\r\n",
            "Were an all-eating shame, and thriftless praise.\r\n",
            "How much more praise deserv’d thy beauty’s use,\r\n",
            "If thou couldst answer ‘This fair child of mine\r\n",
            "Shall sum my count, and make my old excuse,’\r\n",
            "Proving his beauty by succession thine!\r\n",
            "    This were to be new made when thou art old,\r\n",
            "    And see thy blood warm when thou feel’st it cold.\r\n",
            "\r\n",
            "III\r\n",
            "\r\n",
            "Look in thy glass and tell the face thou viewest\r\n",
            "Now is the time that face should form another;\r\n",
            "Whose fresh repair if now thou not renewest,\r\n",
            "Thou dost beguile the world, unbless some mother.\r\n",
            "For where is she so fair whose unear’d womb\r\n",
            "Disdains the tillage of thy husbandry?\r\n",
            "Or who is he so fond will be the tomb,\r\n",
            "Of his self-love to stop posterity?\r\n",
            "Thou art thy mother’s glass and she in thee\r\n",
            "Calls back the lovely April of her prime;\r\n",
            "So thou through windows of thine age shalt see,\r\n",
            "Despite of wrinkles this thy golden time.\r\n",
            "    But if thou live, remember’d not to be,\r\n",
            "    Die single and thine image dies with thee.\r\n",
            "\r\n",
            "IV\r\n",
            "\r\n",
            "Unthrifty loveliness, why dost thou spend\r\n",
            "Upon thyself thy beauty’s legacy?\r\n",
            "Nature’s bequest gives nothing, but doth lend,\r\n",
            "And being frank she lends to those are free:\r\n",
            "Then, beauteous niggard, why dost thou abuse\r\n",
            "The bounteous largess given thee to give?\r\n",
            "Profitless usurer, why dost thou use\r\n",
            "So great a sum of sums, yet canst not live?\r\n",
            "For having traffic with thyself alone,\r\n",
            "Thou of thyself thy sweet self dost deceive:\r\n",
            "Then how when nature calls thee to be gone,\r\n",
            "What acceptable audit canst thou leave?\r\n",
            "    Thy unused beauty must be tombed with thee,\r\n",
            "    Which, used, lives th’ executor to be.\r\n",
            "\r\n",
            "V\r\n",
            "\r\n",
            "Those hours, that with gentle work did frame\r\n",
            "The lovely gaze where every eye doth dwell,\r\n",
            "Will play the tyrants to the very same\r\n",
            "And that unfair which fairly doth excel;\r\n",
            "For never-resting time leads summer on\r\n",
            "To hideous winter, and confounds him there;\r\n",
            "Sap checked with frost, and lusty leaves quite gone,\r\n",
            "Beauty o’er-snowed and bareness every where:\r\n",
            "Then were not summer’s distillation left,\r\n",
            "A liquid prisoner pent in walls of glass,\r\n",
            "Beauty’s effect with beauty were bereft,\r\n",
            "Nor it, nor no remembrance what it was:\r\n",
            "    But flowers distill’d, though they with winter meet,\r\n",
            "    Leese but their show; their substance still lives sweet.\r\n",
            "\r\n",
            "\r\n",
            "VI\r\n",
            "\r\n",
            "Then let not winter’s ragged hand deface,\r\n",
            "In thee thy summer, ere thou be distill’d:\r\n",
            "Make sweet some vial; treasure thou some place\r\n",
            "With beauty’s treasure ere it be self-kill’d.\r\n",
            "That use is not forbidden usury,\r\n",
            "Which happies those that pay the willing loan;\r\n",
            "That’s for thyself to breed another thee,\r\n",
            "Or ten times happier, be it ten for one;\r\n",
            "Ten times thyself were happier than thou art,\r\n",
            "If ten of thine ten times refigur’d thee:\r\n",
            "Then what could death do if thou shouldst depart,\r\n",
            "Leaving thee living in posterity?\r\n",
            "    Be not self-will’d, for thou art much too fair\r\n",
            "    To be death’s conquest and make worms thine heir.\r\n",
            "\r\n",
            "VII\r\n",
            "\r\n",
            "Lo! in the orient when the gracious light\r\n",
            "Lifts up his burning head, each under eye\r\n",
            "Doth homage to his new-appearing sight,\r\n",
            "Serving with looks his sacred majesty;\r\n",
            "And having climb’d the steep-up heavenly hill,\r\n",
            "Resembling strong youth in his middle age,\r\n",
            "Yet mortal looks adore his beauty still,\r\n",
            "Attending on his golden pilgrimage:\r\n",
            "But when from highmost pitch, with weary car,\r\n",
            "Like feeble age, he reeleth from the day,\r\n",
            "The eyes, ’fore duteous, now converted are\r\n",
            "From his low tract, and look another way:\r\n",
            "    So thou, thyself outgoing in thy noon:\r\n",
            "    Unlook’d, on diest unless thou get a son.\r\n",
            "\r\n",
            "VIII\r\n",
            "\r\n",
            "Music to hear, why hear’st thou music sadly?\r\n",
            "Sweets with sweets war not, joy delights in joy:\r\n",
            "Why lov’st thou that which thou receiv’st not gladly,\r\n",
            "Or else receiv’st with pleasure thine annoy?\r\n",
            "If the true concord of well-tuned sounds,\r\n",
            "By unions married, do offend thine ear,\r\n",
            "They do but sweetly chide thee, who confounds\r\n",
            "In singleness the parts that thou shouldst bear.\r\n",
            "Mark how one string, sweet husband to another,\r\n",
            "Str\n"
          ]
        }
      ],
      "source": [
        "   import requests\n",
        "\n",
        "# List of URLs for additional texts (e.g., different Shakespeare plays)\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/1041/1041-0.txt\",  # Hamlet\n",
        "    \"https://www.gutenberg.org/files/152/152-0.txt\",   # Macbeth\n",
        "    \"https://www.gutenberg.org/files/1112/1112-0.txt\"   # Othello\n",
        "]\n",
        "\n",
        "# Initialize an empty string to hold all text\n",
        "all_text = \"\"\n",
        "\n",
        "# Download each text file and append to all_text\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "    all_text += text + \"\\n\\n\"  # Separate texts by newlines\n",
        "\n",
        "# Save combined text to a single file\n",
        "with open(\"combined_shakespeare.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(all_text)\n",
        "\n",
        "    # Print a sample of the text, first 5000 characters\n",
        "print(all_text[:5000])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23a74eca-f1b7-4a46-9a1f-b5806a4ed361",
      "metadata": {
        "tags": [],
        "id": "23a74eca-f1b7-4a46-9a1f-b5806a4ed361"
      },
      "outputs": [],
      "source": [
        "# Filter the dataset\n",
        "filtered_data = [\n",
        "    line\n",
        "    for line in all_text.splitlines()  # Split the combined text into lines\n",
        "    if line.strip() != \"\"  # Remove empty lines\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389c20de-0422-4c48-a7b4-6ee12a7bf0e2",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "389c20de-0422-4c48-a7b4-6ee12a7bf0e2",
        "outputId": "951b953c-ede2-4220-e6f8-76ba29cf3076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5663 lines loaded\n"
          ]
        }
      ],
      "source": [
        "# Count the lines\n",
        "n_lines = len(filtered_data)\n",
        "print(f\"{n_lines} lines loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2e3cf7-e416-460e-874a-0dd9637bca36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b2e3cf7-e416-460e-874a-0dd9637bca36",
        "outputId": "deb787f3-a489-43ca-de9e-f290c447b512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feed’st thy light’s flame with self-substantial fuel,\n"
          ]
        }
      ],
      "source": [
        "example = filtered_data[9]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1",
      "metadata": {
        "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1"
      },
      "source": [
        "## 2. Tokenise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2064fb-5dcc-4657-b470-0928d10e2ddc",
      "metadata": {
        "tags": [],
        "id": "5b2064fb-5dcc-4657-b470-0928d10e2ddc"
      },
      "outputs": [],
      "source": [
        "# Pad the punctuation, to treat them as separate 'words'\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
        "outputId": "1e0adba8-5b8e-433b-a16c-1888637974d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Feed’st thy light’s flame with self - substantial fuel , '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Display an example of a line\n",
        "example_data = text_data[9]\n",
        "example_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1",
      "metadata": {
        "tags": [],
        "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1"
      },
      "outputs": [],
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884c0bcb-0807-45a1-8f7e-a32f2c6fa4de",
      "metadata": {
        "id": "884c0bcb-0807-45a1-8f7e-a32f2c6fa4de"
      },
      "outputs": [],
      "source": [
        "# Create a vectorisation layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6dd34a-d905-497b-926a-405380ebcf98",
      "metadata": {
        "id": "4d6dd34a-d905-497b-926a-405380ebcf98"
      },
      "outputs": [],
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c1c7ce-3cf0-40d4-a3dc-ab7090f69f2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6c1c7ce-3cf0-40d4-a3dc-ab7090f69f2f",
        "outputId": "ed002d71-f388-45f0-c225-313767738a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: ,\n",
            "3: .\n",
            "4: and\n",
            "5: the\n",
            "6: to\n",
            "7: i\n",
            "8: of\n",
            "9: my\n"
          ]
        }
      ],
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
        "outputId": "b42cc42c-5de4-4822-af43-f4d4ff037f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5260   17 4690 1665   19  217   35 3447 5151    2    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c195efb-84c6-4be0-a989-a7542188ad35",
      "metadata": {
        "id": "8c195efb-84c6-4be0-a989-a7542188ad35"
      },
      "source": [
        "## 3. Create the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b",
      "metadata": {
        "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b"
      },
      "outputs": [],
      "source": [
        "# Create the training set of the lines and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
      "metadata": {
        "tags": [],
        "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5"
      },
      "source": [
        "## 4. Build the LSTM <a name=\"build\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "a7abdab5-b5fe-4dc6-df24-d01e5b89d1b0",
        "id": "pPD2v5i9boDF"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m131,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │       \u001b[38;5;34m1,290,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,538,832\u001b[0m (9.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,538,832</span> (9.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,538,832\u001b[0m (9.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,538,832</span> (9.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Input Layer\n",
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "# Embedding Layer\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "\n",
        "# First LSTM Layer: Returns sequences\n",
        "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
        "\n",
        "# Last LSTM Layer\n",
        "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
        "\n",
        "# Output Layer: Dense layer with softmax activation for word prediction\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "\n",
        "# Create model\n",
        "lstm = models.Model(inputs, outputs)\n",
        "\n",
        "# Compile model\n",
        "lstm.compile(optimizer=\"adam\", loss=losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Print model summary\n",
        "lstm.summary()\n"
      ],
      "id": "pPD2v5i9boDF"
    },
    {
      "cell_type": "markdown",
      "id": "35b14665-4359-447b-be58-3fd58ba69084",
      "metadata": {
        "id": "35b14665-4359-447b-be58-3fd58ba69084"
      },
      "source": [
        "## 5. Train the LSTM <a name=\"train\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb1bd3b-6fd9-4536-973e-6375bbcbf16d",
      "metadata": {
        "id": "ffb1bd3b-6fd9-4536-973e-6375bbcbf16d"
      },
      "outputs": [],
      "source": [
        "# Compile LSTM model with Adam optimizer and SparseCategoricalCrossentropy\n",
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm.compile(\"adam\", loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72",
      "metadata": {
        "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72"
      },
      "outputs": [],
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=20, model=None):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }  # <1>\n",
        "\n",
        "    def sample_from(self, probs, temperature):  # <2>\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]  # <3>\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:  # <4>\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x, verbose=0)  # <5>\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)  # <6>\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)  # <7>\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token] if sample_token < len(self.index_to_word) else start_prompt\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"lines:\", max_tokens=100, temperature=0.4) # Increase temperature for more creativity and randomness, decrease temperature for more coherent and predictable text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize starting prompt\n",
        "\n",
        "text_generator = TextGenerator(vocab, model=lstm)\n"
      ],
      "metadata": {
        "id": "6HBPfZS5k3SB"
      },
      "id": "6HBPfZS5k3SB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
        "outputId": "d9d714c1-b9db-47d3-a6da-7e6075f90836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.1316\n",
            "generated text:\n",
            "lines: with the monarch’s flower , the dualist : a ring \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.1316\n",
            "Epoch 2/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1287\n",
            "generated text:\n",
            "lines: , and i wilt be so then i am gone , \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.1287\n",
            "Epoch 3/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1227\n",
            "generated text:\n",
            "lines: , and if thou wilt not i wilt be so ? \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.1227\n",
            "Epoch 4/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1218\n",
            "generated text:\n",
            "lines: , and thou hast be satisfied , but it thou not \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.1218\n",
            "Epoch 5/20\n",
            "\u001b[1m176/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.1198\n",
            "generated text:\n",
            "lines: , and thou love so : and thou art so much : \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.1198\n",
            "Epoch 6/20\n",
            "\u001b[1m176/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.1174\n",
            "generated text:\n",
            "lines: , and thou wilt be ; and she was not so . \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.1174\n",
            "Epoch 7/20\n",
            "\u001b[1m176/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1139\n",
            "generated text:\n",
            "lines: , and thou vnwasht the caue , for i will not \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.1139\n",
            "Epoch 8/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.1108\n",
            "generated text:\n",
            "lines: , and thou art not from the world of be showne , \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.1109\n",
            "Epoch 9/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.1103\n",
            "generated text:\n",
            "lines: , and thou art , and say , for my love in thee , \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - loss: 0.1103\n",
            "Epoch 10/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1044\n",
            "generated text:\n",
            "lines: in the painted day of my verse ; \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.1044\n",
            "Epoch 11/20\n",
            "\u001b[1m176/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.1043\n",
            "generated text:\n",
            "lines: , and they will be satisfied : \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.1044\n",
            "Epoch 12/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1043\n",
            "generated text:\n",
            "lines: , and the little thing i am so much : \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - loss: 0.1043\n",
            "Epoch 13/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1002\n",
            "generated text:\n",
            "lines: , and thou art , i art so much in my sight \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.1002\n",
            "Epoch 14/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0972\n",
            "generated text:\n",
            "lines: , and yet thou wilt be , and thou art it ? \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.0972\n",
            "Epoch 15/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0945\n",
            "generated text:\n",
            "lines: , and you do fourteene , thou beautie hands , \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.0945\n",
            "Epoch 16/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0953\n",
            "generated text:\n",
            "lines: is the volume of young paris : \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - loss: 0.0953\n",
            "Epoch 17/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0910\n",
            "generated text:\n",
            "lines: , and thou art : thy lovely thing that thy heart \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.0910\n",
            "Epoch 18/20\n",
            "\u001b[1m176/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0895\n",
            "generated text:\n",
            "lines: , and thou art : and therein ; it in her : \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - loss: 0.0895\n",
            "Epoch 19/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0861\n",
            "generated text:\n",
            "lines: , and they have confess’d and they , \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.0861\n",
            "Epoch 20/20\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0852\n",
            "generated text:\n",
            "lines: with a terror of the ground , \n",
            "\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - loss: 0.0852\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ba36359a8f0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Train the LSTM model on the dataset\n",
        "\n",
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJSZJxsjrhNL"
      },
      "id": "IJSZJxsjrhNL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d64e02d2-84dc-40c8-8446-40c09adf1e20",
      "metadata": {
        "id": "d64e02d2-84dc-40c8-8446-40c09adf1e20"
      },
      "source": [
        "## 6. Generate text using the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad23adb-3ec9-4e9a-9a59-b9f9bafca649",
      "metadata": {
        "id": "4ad23adb-3ec9-4e9a-9a59-b9f9bafca649"
      },
      "outputs": [],
      "source": [
        "# Display predicted words with their probabilities for each prompt\n",
        "\n",
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
      "metadata": {
        "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb8cada-fff8-4d1f-efb9-f9c7a49f7e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "To be, or not to be in the world ?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generating text with a Shakespearean prompt\n",
        "info = text_generator.generate(\n",
        "     \"To be, or not to be\", max_tokens=10, temperature=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df72866-b483-4489-8e26-d5e1466410fa",
      "metadata": {
        "tags": [],
        "id": "9df72866-b483-4489-8e26-d5e1466410fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe49866-728c-4b51-87a3-d5defd91d52e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: To be, or not to be\n",
            "past:   \t6.28%\n",
            "our:   \t4.54%\n",
            "ill:   \t3.73%\n",
            "false:   \t3.3%\n",
            "more:   \t3.21%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be, or not to be in\n",
            "the:   \t32.13%\n",
            "his:   \t15.14%\n",
            "thy:   \t12.14%\n",
            "a:   \t9.73%\n",
            "her:   \t6.62%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be, or not to be in the\n",
            "world:   \t11.83%\n",
            "vault:   \t9.71%\n",
            "spring:   \t4.26%\n",
            "living:   \t4.12%\n",
            "monument:   \t3.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be, or not to be in the world\n",
            ",:   \t92.85%\n",
            "?:   \t5.19%\n",
            "::   \t1.8%\n",
            ";:   \t0.09%\n",
            ".:   \t0.02%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "562e1fe8-cbcb-438f-9637-2f2a6279c924",
      "metadata": {
        "id": "562e1fe8-cbcb-438f-9637-2f2a6279c924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8738da3f-de7e-426e-cc9f-b276f2215519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Shall I compare thee to a summer's day? , \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generating text with a Shakespearean prompt\n",
        "info = text_generator.generate(\n",
        "    \"Shall I compare thee to a summer's day?\", max_tokens=10, temperature=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56356f21-04ac-40e5-94ff-291eca6a7054",
      "metadata": {
        "id": "56356f21-04ac-40e5-94ff-291eca6a7054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ce4f3e-ca17-4268-91ad-98f623515fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: Shall I compare thee to a summer's day?\n",
            ",:   \t100.0%\n",
            "::   \t0.0%\n",
            "?:   \t0.0%\n",
            ":   \t0.0%\n",
            ".:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Shall I compare thee to a summer's day? ,\n",
            ":   \t100.0%\n",
            "/:   \t0.0%\n",
            "’:   \t0.0%\n",
            ".:   \t0.0%\n",
            "is:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e434497-07f3-4989-a68d-3e31cf8fa4fe",
      "metadata": {
        "id": "2e434497-07f3-4989-a68d-3e31cf8fa4fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d067db1-262f-44d0-ab4d-ea452dbe5d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "All the world's a stage , and\n",
            "\n",
            "\n",
            "PROMPT: All the world's a stage\n",
            ",:   \t99.38%\n",
            "::   \t0.58%\n",
            "?:   \t0.02%\n",
            ";:   \t0.01%\n",
            "of:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: All the world's a stage ,\n",
            "and:   \t52.98%\n",
            "or:   \t4.69%\n",
            ":   \t3.1%\n",
            "&:   \t2.45%\n",
            "but:   \t2.24%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generating text with a Shakespearean prompt\n",
        "info = text_generator.generate(\n",
        "    \"All the world's a stage\", max_tokens=7, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}